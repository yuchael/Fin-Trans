import os
from pathlib import Path
from typing import TypedDict, Literal, Any
from dotenv import load_dotenv

# ë²¡í„° DB ë° LLM (LangChain í˜¸í™˜ ìœ ì§€)
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, START, END

from rag_agent.web_search_rag import WebSearchRAG

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()

# ê²½ë¡œ ì„¤ì •
CURRENT_FILE_PATH = Path(__file__).resolve()
PROJECT_ROOT = CURRENT_FILE_PATH.parent.parent
PROMPT_DIR = CURRENT_FILE_PATH.parent / "prompt" / "finrag"

CHROMA_DB_PATH = PROJECT_ROOT / "data" / "financial_terms"
COLLECTION_NAME = "financial_terms"

SIMILARITY_THRESHOLD = 0.6
WEB_SEARCH_KEYWORDS = ["í˜„ì¬", "ìµœì‹ ", "ì˜¤ëŠ˜", "ì£¼ê°€", "ì‹œì„¸", "ë‰´ìŠ¤", "ì „ë§", "ë‚ ì”¨", "ê²€ìƒ‰í•´ì¤˜", "ì–¼ë§ˆì•¼"]

# ì „ì—­ ë³€ìˆ˜
vectorstore = None
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
web_rag = WebSearchRAG()

def load_prompt(filename: str) -> str:
    file_path = PROMPT_DIR / filename
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        print(f"âŒ [Error] í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return "{context}\n{question}"

def load_knowledge_base():
    """ChromaDB ì—°ê²° ì„¤ì •"""
    global vectorstore
    if vectorstore is not None:
        return
    print("â³ [RAG] ChromaDB ì—°ê²° ì¤‘...")
    try:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        vectorstore = Chroma(
            persist_directory=str(CHROMA_DB_PATH),
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME,
            collection_metadata={"hnsw:space": "l2"},
        )
        print(f"âœ… ChromaDB ì—°ê²° ì™„ë£Œ (Metirc: L2, ê²½ë¡œ: {CHROMA_DB_PATH})")
    except Exception as e:
        print(f"âŒ ChromaDB ì—°ê²° ì˜¤ë¥˜: {e}")
        vectorstore = None

def format_web_result(web_result, original_query, translated_query):
    citations = [f"- **{src['title']}**: {src['url']}" for src in web_result.get("sources", [])]
    citation_text = "\n".join(citations) if citations else "- ì¶œì²˜ ì •ë³´ ì—†ìŒ"
    return f"""
### ğŸŒ ì§ˆë¬¸
- **Original**: {original_query if original_query else translated_query}
- **Translated**: {translated_query}

### ğŸŒ FinBotì˜ ì›¹ ê²€ìƒ‰ ë‹µë³€
{web_result['answer']}

---
### ğŸ“š ì°¸ê³  ì›¹ì‚¬ì´íŠ¸
{citation_text}
"""

# ---------------------------------------------------------
# [LangGraph] FinRAG ìƒíƒœ
# ---------------------------------------------------------
class FinRAGState(TypedDict, total=False):
    korean_query: str
    original_query: str
    use_web: bool
    relevant_docs: list
    context_text: str
    citations: list
    final_output: str

# ---------------------------------------------------------
# [LangGraph] ë…¸ë“œ
# ---------------------------------------------------------
def node_route(state: FinRAGState) -> dict:
    korean_query = state["korean_query"]
    use_web = any(kw in korean_query for kw in WEB_SEARCH_KEYWORDS)
    if use_web:
        print(f"ğŸš€ [FinRAG] ì‹¤ì‹œê°„ í‚¤ì›Œë“œ ê°ì§€ -> ì›¹ ê²€ìƒ‰ ì „í™˜: '{korean_query}'")
    return {"use_web": use_web}

def node_web_search(state: FinRAGState) -> dict:
    korean_query = state["korean_query"]
    original_query = state.get("original_query")
    web_result = web_rag.web_search(korean_query)
    final_output = format_web_result(web_result, original_query, korean_query)
    return {"final_output": final_output}

def node_db_retrieve(state: FinRAGState) -> dict:
    global vectorstore
    if vectorstore is None:
        load_knowledge_base()
    korean_query = state["korean_query"]
    relevant_docs = []
    if vectorstore:
        try:
            results = vectorstore.similarity_search_with_score(korean_query, k=5)
            print(f"ğŸ” [Search] '{korean_query}' DB ê²€ìƒ‰ ìˆ˜í–‰")
            for doc, score in results:
                if score <= SIMILARITY_THRESHOLD:
                    relevant_docs.append((doc, score))
                    print(f"   âœ… ì±„íƒ: {doc.metadata.get('word')} (ê±°ë¦¬: {score:.4f})")
                else:
                    print(f"   âŒ ì œì™¸: {doc.metadata.get('word')} (ê±°ë¦¬: {score:.4f} > {SIMILARITY_THRESHOLD})")
            relevant_docs = relevant_docs[:3]
        except Exception as e:
            print(f"âš ï¸ DB ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
    return {"relevant_docs": relevant_docs}

def node_web_fallback(state: FinRAGState) -> dict:
    print(f"âš ï¸ [FinRAG] ë‚´ë¶€ DBì— ê´€ë ¨ ì •ë³´ ì—†ìŒ (ìœ íš¨ ë¬¸ì„œ 0ê°œ) -> ì›¹ ê²€ìƒ‰ ìë™ ì „í™˜")
    return node_web_search(state)

def node_db_answer(state: FinRAGState) -> dict:
    korean_query = state["korean_query"]
    original_query = state.get("original_query")
    relevant_docs = state.get("relevant_docs") or []
    context_text = ""
    citations = []
    for doc, score in relevant_docs:
        word = doc.metadata.get("word", "Term")
        raw_content = doc.page_content
        definition = raw_content.split(":", 1)[1].strip() if ":" in raw_content else raw_content
        context_text += f"- **{word}**: {definition}\n"
        citations.append(f"- **{word}**: {definition[:60]}... (ê±°ë¦¬: {score:.4f})")

    system_template = load_prompt("finrag_01_system.md")
    rag_prompt = PromptTemplate.from_template(system_template)
    rag_chain = rag_prompt | llm | StrOutputParser()
    try:
        ai_answer = rag_chain.invoke({"context": context_text, "question": korean_query})
    except Exception as e:
        ai_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ({e})"

    final_output = f"""
### ğŸŒ ì§ˆë¬¸
- **Original**: {original_query if original_query else korean_query}
- **Translated**: {korean_query}

### ğŸ’¡ FinBotì˜ ë‹µë³€
{ai_answer}

---
### ğŸ“š ë‚´ë¶€ ì°¸ê³  ë¬¸í—Œ
{chr(10).join(citations)}
"""
    return {"final_output": final_output}

def route_after_start(state: FinRAGState) -> Literal["web_search", "db_retrieve"]:
    return "web_search" if state.get("use_web") else "db_retrieve"

def route_after_db(state: FinRAGState) -> Literal["web_fallback", "db_answer"]:
    return "web_fallback" if not (state.get("relevant_docs")) else "db_answer"

# ---------------------------------------------------------
# ê·¸ë˜í”„ ë¹Œë“œ
# ---------------------------------------------------------
_finrag_graph = None

def _get_finrag_graph():
    global _finrag_graph
    if _finrag_graph is None:
        builder = StateGraph(FinRAGState)
        builder.add_node("route", node_route)
        builder.add_node("web_search", node_web_search)
        builder.add_node("db_retrieve", node_db_retrieve)
        builder.add_node("web_fallback", node_web_fallback)
        builder.add_node("db_answer", node_db_answer)

        builder.add_edge(START, "route")
        builder.add_conditional_edges("route", route_after_start, {"web_search": "web_search", "db_retrieve": "db_retrieve"})
        builder.add_edge("web_search", END)
        builder.add_conditional_edges("db_retrieve", route_after_db, {"web_fallback": "web_fallback", "db_answer": "db_answer"})
        builder.add_edge("web_fallback", END)
        builder.add_edge("db_answer", END)
        _finrag_graph = builder.compile()
    return _finrag_graph

def get_rag_answer(korean_query, original_query=None):
    if vectorstore is None:
        load_knowledge_base()
    graph = _get_finrag_graph()
    initial: FinRAGState = {"korean_query": korean_query, "original_query": original_query}
    result = graph.invoke(initial)
    return result.get("final_output", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    load_knowledge_base()
    print(get_rag_answer("ê¸ˆë¦¬ê°€ ë­ì•¼?"))
    print("-" * 50)
    print(get_rag_answer("í˜„ì¬ ì‚¼ì„±ì „ì ì£¼ê°€ ì•Œë ¤ì¤˜"))
    print("-" * 50)
    print(get_rag_answer("ì•„ì´ìœ  ìµœì‹  ì•¨ë²” ë­ì•¼?"))

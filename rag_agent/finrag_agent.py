import os
from pathlib import Path
from dotenv import load_dotenv

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()

# ê²½ë¡œ ì„¤ì •
CURRENT_FILE_PATH = Path(__file__).resolve() 
PROJECT_ROOT = CURRENT_FILE_PATH.parent.parent 
PROMPT_DIR = CURRENT_FILE_PATH.parent / "prompt" / "finrag"

# ChromaDB ë°ì´í„° ê²½ë¡œ
CHROMA_DB_PATH = PROJECT_ROOT / "data" / "financial_terms"
COLLECTION_NAME = "financial_terms"

# [ì„¤ì •] ê²€ìƒ‰ í’ˆì§ˆì„ ìœ„í•œ ì„ê³„ê°’ (Threshold)
# ê±°ë¦¬(Distance) ê¸°ì¤€ì´ë¯€ë¡œ, ì´ ê°’ë³´ë‹¤ 'ì‘ì•„ì•¼' ìœ ì‚¬í•œ ë¬¸ì„œì…ë‹ˆë‹¤.
# L2 Distance ê¸°ì¤€: 0.5 ~ 0.8 ì‚¬ì´ ê¶Œì¥ (ë°ì´í„° ë¶„í¬ì— ë”°ë¼ ì¡°ì ˆ í•„ìš”)
SIMILARITY_THRESHOLD = 0.6

# ì „ì—­ ë³€ìˆ˜
vectorstore = None
llm = ChatOpenAI(model="gpt-5-mini")

def load_prompt(filename: str) -> str:
    """MD íŒŒì¼ì„ ì½ì–´ì„œ ë¬¸ìì—´ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    file_path = PROMPT_DIR / filename
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        print(f"âŒ [Error] í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return "{context}\n{question}"

def load_knowledge_base():
    """ChromaDB ì—°ê²° ì„¤ì •"""
    global vectorstore
    if vectorstore is not None: return

    print("â³ [RAG] ChromaDB ì—°ê²° ì¤‘...")
    try:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        
        # [ë³€ê²½] ê±°ë¦¬ ì¸¡ì • ë°©ì‹ ë³€ê²½ (cosine -> l2)
        # ì£¼ì˜: DBë¥¼ ìƒˆë¡œ ìƒì„±í•´ì•¼ ì™„ë²½í•˜ê²Œ ì ìš©ë©ë‹ˆë‹¤.
        vectorstore = Chroma(
            persist_directory=str(CHROMA_DB_PATH),
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME,
            collection_metadata={"hnsw:space": "l2"} 
        )
        print(f"âœ… ChromaDB ì—°ê²° ì™„ë£Œ (Metirc: L2, ê²½ë¡œ: {CHROMA_DB_PATH})")
        
    except Exception as e:
        print(f"âŒ ChromaDB ì—°ê²° ì˜¤ë¥˜: {e}")
        vectorstore = None

def get_rag_answer(korean_query, original_query=None):
    if vectorstore is None: load_knowledge_base()

    # 1. ë¬¸ì„œ ê²€ìƒ‰ (Score í¬í•¨)
    relevant_docs = []
    if vectorstore:
        # ë„‰ë„‰í•˜ê²Œ 5ê°œë¥¼ ê°€ì ¸ì˜¨ ë’¤ í•„í„°ë§
        results = vectorstore.similarity_search_with_score(korean_query, k=5)
        
        # [ì¶”ê°€] Threshold í•„í„°ë§ ë¡œì§
        print(f"ğŸ” [Search] '{korean_query}' ê²€ìƒ‰ ê²°ê³¼ (Threshold: {SIMILARITY_THRESHOLD})")
        for doc, score in results:
            # L2 DistanceëŠ” 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬í•¨
            if score <= SIMILARITY_THRESHOLD:
                relevant_docs.append((doc, score))
                print(f"   âœ… ì±„íƒ: {doc.metadata.get('word')} (ê±°ë¦¬: {score:.4f})")
            else:
                print(f"   âŒ ì œì™¸: {doc.metadata.get('word')} (ê±°ë¦¬: {score:.4f} > {SIMILARITY_THRESHOLD})")
        
        # ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
        relevant_docs = relevant_docs[:3]
    
    # 2. ì»¨í…ìŠ¤íŠ¸ ë° ì¶œì²˜(Citation) êµ¬ì„±
    context_text = ""
    citations = []
    
    if relevant_docs:
        for doc, score in relevant_docs:
            # L2 ê±°ë¦¬ì¼ ë•ŒëŠ” ìœ ì‚¬ë„(%) í‘œí˜„ì´ ì• ë§¤í•˜ë¯€ë¡œ ê±°ë¦¬ê°’ ìì²´ë¥¼ í‘œê¸°í•˜ê±°ë‚˜ ìƒëµ
            # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ ê±°ë¦¬(Distance)ë¥¼ ê·¸ëŒ€ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
            
            word = doc.metadata.get("word", "Term")
            raw_content = doc.page_content
            
            definition = raw_content.split(":", 1)[1].strip() if ":" in raw_content else raw_content
            
            context_text += f"- **{word}**: {definition}\n"
            citations.append(f"- **{word}**: {definition[:60]}... (ê±°ë¦¬: {score:.4f})")
            
    else:
        print("âš ï¸ [Retrieved Docs]: ìœ íš¨í•œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (Threshold ë¯¸ë‹¬)")
        context_text = "" 
        citations.append("- ê´€ë ¨ëœ ë‚´ë¶€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ê²€ìƒ‰ ê¸°ì¤€ ë¯¸ë‹¬).")

    # 3. í”„ë¡¬í”„íŠ¸ ë¡œë”© ë° ì²´ì¸ ìƒì„±
    system_template = load_prompt("finrag_01_system.md")
    rag_prompt = PromptTemplate.from_template(system_template)
    rag_chain = rag_prompt | llm | StrOutputParser()

    # 4. LLM í˜¸ì¶œ
    try:
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´(context_textê°€ ë¹„ì—ˆìœ¼ë©´) í”„ë¡¬í”„íŠ¸ì—ì„œ Fallback ì²˜ë¦¬ê°€ ë˜ë„ë¡ ìœ ë„
        ai_answer = rag_chain.invoke({
            "context": context_text if context_text else "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.",
            "question": korean_query
        })
    except Exception as e:
        ai_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ({e})"

    # 5. ìµœì¢… ì¶œë ¥ í¬ë§·íŒ…
    final_output = f"""
### ğŸŒ ì§ˆë¬¸ (Question)
- **Original**: {original_query if original_query else korean_query}
- **Translated**: {korean_query}

### ğŸ’¡ FinBotì˜ ë‹µë³€
{ai_answer}

---
### ğŸ“š ì°¸ê³  ë¬¸í—Œ (References)
{chr(10).join(citations)}
    """
    
    return final_output

if __name__ == "__main__":
    load_knowledge_base()
    # í…ŒìŠ¤íŠ¸
    print(get_rag_answer("ê¸ˆë¦¬ê°€ ë­ì•¼?"))
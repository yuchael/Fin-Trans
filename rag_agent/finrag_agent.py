import os
import json
import numpy as np
import pandas as pd
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv
from utils.handle_sql import get_data  # DB ì—°ê²° ëª¨ë“ˆ

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ì „ì—­ ë³€ìˆ˜
df = None
embedding_matrix = None

# í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
CURRENT_FILE_PATH = Path(__file__).resolve() 
PROJECT_ROOT = CURRENT_FILE_PATH.parent.parent 
PROMPT_PATH = PROJECT_ROOT / "utils" / "system_prompt.md" 

def load_knowledge_base():
    """DB ë°ì´í„° ë¡œë”©"""
    global df, embedding_matrix
    if df is not None: return

    print("â³ [RAG] ê¸ˆìœµ ì§€ì‹ ë² ì´ìŠ¤ ë¡œë”© ì¤‘...")
    try:
        rows = get_data("SELECT word, definition, embedding FROM terms")
        df = pd.DataFrame(rows)
        
        if df.empty:
            print("âš ï¸ ë°ì´í„° ì—†ìŒ.")
            return

        df['embedding'] = df['embedding'].apply(json.loads)
        embedding_matrix = np.vstack(df['embedding'].values)
        print(f"âœ… ë¡œë”© ì™„ë£Œ ({len(df)}ê°œ)")
    except Exception as e:
        print(f"âŒ ë¡œë”© ì˜¤ë¥˜: {e}")
        df = None

def get_embedding(text):
    return client.embeddings.create(input=[text], model="text-embedding-3-small").data[0].embedding

def search_docs(query_text, top_k=3):
    if df is None: return pd.DataFrame()
    
    query_vec = get_embedding(query_text)
    similarities = np.dot(embedding_matrix, query_vec) / (
        np.linalg.norm(embedding_matrix, axis=1) * np.linalg.norm(query_vec)
    )
    df['similarity'] = similarities
    # ìœ ì‚¬ë„ 0.3 ì´ìƒì¸ ê²ƒë§Œ í•„í„°ë§ (ë„ˆë¬´ ì—‰ëš±í•œ ë¬¸ì„œ ì œì™¸)
    return df[df['similarity'] >= 0.3].sort_values('similarity', ascending=False).head(top_k)

def read_prompt_file():
    """MD íŒŒì¼ì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì½ê¸°"""
    try:
        with open(PROMPT_PATH, "r", encoding="utf-8") as f:
            return f.read()
    except Exception:
        return "You are a helpful assistant." # íŒŒì¼ ì—†ì„ ì‹œ ê¸°ë³¸ê°’

# ğŸ”¥ í•µì‹¬ í•¨ìˆ˜: ì¸ìì— original_query ì¶”ê°€
def get_rag_answer(korean_query, original_query=None):
    if df is None: load_knowledge_base()

    # 1. ë¬¸ì„œ ê²€ìƒ‰
    relevant_docs = search_docs(korean_query, top_k=3)
    
    # 2. ì»¨í…ìŠ¤íŠ¸ ë° ì¶œì²˜(Citation) êµ¬ì„±
    context_text = ""
    citations = []
    
    if not relevant_docs.empty:
        for idx, row in relevant_docs.iterrows():
            context_text += f"Term: {row['word']}\nDefinition: {row['definition']}\n\n"
            citations.append(f"- **{row['word']}**: {row['definition'][:50]}... (ìœ ì‚¬ë„: {row['similarity']:.2f})")
    else:
        context_text = "ê´€ë ¨ëœ DB ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì§€ì‹ì„ í™œìš©í•˜ì„¸ìš”."
        citations.append("- ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # 3. í”„ë¡¬í”„íŠ¸ ë¡œë”© ë° êµ¬ì„±
    system_template = read_prompt_file()
    formatted_system_prompt = system_template.format(context=context_text)

    # 4. LLM í˜¸ì¶œ
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": formatted_system_prompt},
            {"role": "user", "content": f"ì§ˆë¬¸ì— ëŒ€í•´ ì´ˆë“±í•™ìƒ ì„ ìƒë‹˜ì²˜ëŸ¼ í•µì‹¬ë§Œ ì§§ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”: {korean_query}"}
        ],
        temperature=0.3
    )
    
    ai_answer = response.choices[0].message.content.strip()

    # 5. ìµœì¢… ì¶œë ¥ í¬ë§·íŒ… (ìš”ì²­í•˜ì‹  ë¶€ë¶„)
    final_output = f"""
### ğŸŒ ì§ˆë¬¸ (Question)
- **Original**: {original_query if original_query else korean_query}
- **Translated**: {korean_query}

### ğŸ’¡ ì„ ìƒë‹˜ì˜ ë‹µë³€
{ai_answer}

---
### ğŸ“š ì°¸ê³  ë¬¸í—Œ (References)
{chr(10).join(citations)}
    """
    
    return final_output

if __name__ == "__main__":
    load_knowledge_base()
    print(get_rag_answer("ì§‘ì„ êµ¬í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í•´?", "How can I find a house?"))
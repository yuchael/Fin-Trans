import os
import pdfplumber
import pandas as pd
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
import re

print("ğŸš€ [ìµœì¢…] ê¸ˆìœµ ìš©ì–´ PDF -> MySQL DB ì ì¬ ì‹œì‘ (Strict Match Mode)...")

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼)
load_dotenv()

DB_USER = os.getenv("DB_USER", "root")
DB_PASSWORD = os.getenv("DB_PASSWORD", "")
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = os.getenv("DB_PORT", "3306")
DB_NAME = os.getenv("DB_NAME", "fintech_agent")

PDF_FILE_PATH = os.path.join(BASE_DIR, "..", "data", "economic_terms.pdf")

# í˜ì´ì§€ ì„¤ì •
INDEX_START_PAGE = 5   
INDEX_END_PAGE = 16    
BODY_START_PAGE = 17   

# 2. DB ì—°ê²° ì—”ì§„ ìƒì„±
def get_db_engine():
    # mysql+pymysql://ì‚¬ìš©ì:ë¹„ë²ˆ@í˜¸ìŠ¤íŠ¸:í¬íŠ¸/DBëª…
    db_url = f"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}?charset=utf8mb4"
    return create_engine(db_url)

# 3. í…Œì´ë¸” ì´ˆê¸°í™” (ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì¬ìƒì„±)
def init_db_table():
    try:
        engine = get_db_engine()
        with engine.connect() as conn:
            # ê¸°ì¡´ í…Œì´ë¸”ì´ ìˆë‹¤ë©´ ì‚­ì œ (í…ŒìŠ¤íŠ¸ìš©)
            conn.execute(text("DROP TABLE IF EXISTS terms"))
            
            # í…Œì´ë¸” ìƒì„± (definitionì€ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìœ„í•´ LONGTEXT ì‚¬ìš©)
            create_sql = """
            CREATE TABLE terms (
                id INT AUTO_INCREMENT PRIMARY KEY,
                word VARCHAR(255) NOT NULL,
                definition LONGTEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
            """
            conn.execute(text(create_sql))
            print("âœ… DB í…Œì´ë¸”(terms) ì´ˆê¸°í™” ì™„ë£Œ.")
    except Exception as e:
        print(f"âŒ DB ì ‘ì† ë˜ëŠ” í…Œì´ë¸” ìƒì„± ì˜¤ë¥˜: {e}")
        exit()

# 4. ì •ê·œí™” í•¨ìˆ˜ (ë¹„êµìš©: ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±°)
def normalize(text):
    if not text: return ""
    return re.sub(r'[\s\(\)\[\]\-\.,ï½¥ãƒ»/]', '', text)

# 5. [1ë‹¨ê³„] ëª©ì°¨ ì •ë°€ ì¶”ì¶œ (ë…¸ì´ì¦ˆ ì œê±° + í•©ì¹˜ê¸°)
def extract_master_terms():
    print("ğŸ“– [1ë‹¨ê³„] ëª©ì°¨ ì •ë°€ ì¶”ì¶œ ì¤‘...")
    term_list = []
    
    # ëª©ì°¨ íŒ¨í„´: "ìš©ì–´" + "ì ë“¤" + "ìˆ«ì"
    index_pattern = re.compile(r'^(?P<term>.*?)\s*[ï½¥ãƒ»\.]+\s*\d+$')
    
    # ğŸ”¥ [ìˆ˜ì •] 15ê°œ ëˆ„ë½ ì›ì¸ì´ì—ˆë˜ í—¤ë” ë…¸ì´ì¦ˆ ì œê±° íŒ¨í„´
    noise_prefix_pattern = re.compile(r'^(ê²½ì œê¸ˆìœµìš©ì–´\s*\d*ì„ |ë³´ê¸°|ì°¸ê³ )\s*')

    with pdfplumber.open(PDF_FILE_PATH) as pdf:
        for i in range(INDEX_START_PAGE - 1, INDEX_END_PAGE):
            page = pdf.pages[i]
            width = page.width
            height = page.height
            
            # 2ë‹¨ ë¶„ë¦¬
            left_box = (0, 60, width / 2, height - 50)
            right_box = (width / 2, 60, width, height - 50)
            
            for box in [left_box, right_box]:
                try:
                    text = page.crop(box).extract_text()
                except: continue
                if not text: continue
                
                lines = text.split('\n')
                prev_line = ""
                
                for line in lines:
                    # ê¸°ë³¸ ë…¸ì´ì¦ˆ ì œê±°
                    clean_line = line.replace("ì°¾ì•„ë³´ê¸°", "").replace("ì°¾ì•„ë³´", "").replace("â™", "").strip()
                    if not clean_line: continue
                    
                    # ğŸ”¥ í—¤ë” ë…¸ì´ì¦ˆ ì œê±° (ì´ê²Œ ìˆì–´ì•¼ 'ì ì¬GDPì„±ì¥ë¥ ' ë“±ì´ ì‚´ì•„ë‚¨ìŒ)
                    clean_line = noise_prefix_pattern.sub('', clean_line)

                    match = index_pattern.match(clean_line)
                    if match:
                        current_term = match.group('term').strip()
                        if prev_line:
                            # ì¤„ë°”ê¿ˆ ìš©ì–´ í•©ì¹˜ê¸° (ê³µë°± ì—†ì´)
                            full_term = f"{prev_line}{current_term}"
                            term_list.append(full_term)
                            prev_line = "" 
                        else:
                            if len(current_term) > 1:
                                term_list.append(current_term)
                    else:
                        if len(clean_line) > 1 and not clean_line.isdigit():
                            prev_line = clean_line

    # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€ X -> set í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜)
    unique_terms = list(dict.fromkeys(term_list))
    print(f"âœ… ëª©ì°¨ ì¶”ì¶œ ì™„ë£Œ: {len(unique_terms)}ê°œ ìš©ì–´ ê¸°ì¤€ í™•ë³´.")
    return unique_terms

# 6. [2ë‹¨ê³„] ë³¸ë¬¸ íŒŒì‹± ë° DB ì ì¬
def parse_and_insert_db():
    # DB ì´ˆê¸°í™”
    init_db_table()
    
    # ëª©ì°¨ ê°€ì ¸ì˜¤ê¸°
    master_terms = extract_master_terms()
    
    # ë¹„êµ ì†ë„ë¥¼ ìœ„í•´ ì •ê·œí™”ëœ ì…‹(Set) ìƒì„±
    normalized_master_set = set(normalize(t) for t in master_terms)
    
    print(f"ğŸ“‚ [2ë‹¨ê³„] ë³¸ë¬¸ ë¶„ì„ ë° DB ì ì¬ ì‹œì‘ (ì—„ê²©í•œ ì¼ì¹˜)...")
    
    data_list = [] # ëŒ€ëŸ‰ Insertë¥¼ ìœ„í•œ ë²„í¼
    
    with pdfplumber.open(PDF_FILE_PATH) as pdf:
        current_title = ""
        current_body = ""
        
        for i, page in enumerate(pdf.pages):
            current_page_num = i + 1
            if current_page_num < BODY_START_PAGE: continue
            
            width, height = page.width, page.height
            try:
                # ë³¸ë¬¸ ì˜ì—­ í¬ë¡­
                cropped = page.crop((0, 80, width, height - 70))
                text = cropped.extract_text()
            except: continue

            if not text: continue

            lines = text.split('\n')
            for line in lines:
                clean_line = line.strip()
                if len(clean_line) < 1: continue
                if "ì—°ê´€ê²€ìƒ‰ì–´" in clean_line: continue

                # ì •ê·œí™”
                norm_line = normalize(clean_line)
                
                # ğŸ”¥ [ì—„ê²©í•œ ë¡œì§] ì •ê·œí™”ëœ ë¼ì¸ì´ ëª©ì°¨ ì…‹ì— 'ì •í™•íˆ' ìˆëŠ”ê°€?
                # (í¬í•¨ X, ì¼ì¹˜ O) -> ë¬¸ì¥ ì¤‘ê°„ì˜ ë‹¨ì–´ ë•Œë¬¸ì— ëŠê¸°ëŠ” í˜„ìƒ ë°©ì§€
                is_title = norm_line in normalized_master_set

                if is_title:
                    # ì´ì „ ìš©ì–´ ì €ì¥ (ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€)
                    if current_title and current_body:
                        data_list.append({
                            "word": current_title,
                            "definition": current_body.strip()
                        })
                    
                    # ìƒˆ ìš©ì–´ ì‹œì‘
                    current_title = clean_line
                    current_body = "" # ì œëª© ì¤„ì€ ë³¸ë¬¸ì— ë„£ì§€ ì•ŠìŒ
                else:
                    # ë³¸ë¬¸ ë‚´ìš© ì¶”ê°€
                    if "PDF.js" not in clean_line and not clean_line.isdigit():
                        current_body += " " + clean_line

            if current_page_num % 50 == 0:
                print(f"   ... {current_page_num}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘")

        # ë§ˆì§€ë§‰ ìš©ì–´ ì¶”ê°€
        if current_title and current_body:
            data_list.append({
                "word": current_title,
                "definition": current_body.strip()
            })

    # DBì— ì¼ê´„ ì €ì¥ (Bulk Insert)
    if data_list:
        print(f"ğŸ’¾ ì´ {len(data_list)}ê°œ ë°ì´í„°ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤...")
        df = pd.DataFrame(data_list)
        engine = get_db_engine()
        
        # pandas to_sql ì‚¬ìš© (ë¹ ë¥´ê³  ê°„í¸í•¨)
        df.to_sql(name='terms', con=engine, if_exists='append', index=False)
        print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! ì„±ê³µì ìœ¼ë¡œ DBì— ì ì¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    parse_and_insert_db()
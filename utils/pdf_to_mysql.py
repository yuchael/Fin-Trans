import os
import pdfplumber
import re
import pandas as pd
from dotenv import load_dotenv


from utils.handle_sql import execute_query, execute_many

print("ğŸš€ [ìµœì¢…] ê¸ˆìœµ ìš©ì–´ PDF -> MySQL DB ì ì¬ ì‹œì‘ (Strict Match Mode)...")

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# íŒŒì¼ ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__)) # í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€
PDF_FILE_PATH = os.path.join(BASE_DIR, "..", "data", "economic_terms.pdf")

# í˜ì´ì§€ ì„¤ì •
INDEX_START_PAGE = 5   
INDEX_END_PAGE = 16    
BODY_START_PAGE = 17   

# 3. í…Œì´ë¸” ì´ˆê¸°í™” (ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì¬ìƒì„±)
def init_db_table():
    try:
        print("âš™ï¸ DB í…Œì´ë¸”(terms) ì´ˆê¸°í™” ì¤‘...")
        # [ë³€ê²½] execute_queryë¥¼ ì‚¬ìš©í•˜ì—¬ DDL ì‹¤í–‰
        execute_query("DROP TABLE IF EXISTS terms")
        
        create_sql = """
        CREATE TABLE terms (
            id INT AUTO_INCREMENT PRIMARY KEY,
            word VARCHAR(255) NOT NULL,
            definition LONGTEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        """
        execute_query(create_sql)
        print("âœ… DB í…Œì´ë¸”(terms) ì´ˆê¸°í™” ì™„ë£Œ.")
    except Exception as e:
        print(f"âŒ DB ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        exit()

# 4. ì •ê·œí™” í•¨ìˆ˜ (ë¹„êµìš©: ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±°)
def normalize(text):
    if not text: return ""
    return re.sub(r'[\s\(\)\[\]\-\.,ï½¥ãƒ»/]', '', text)

# 5. [1ë‹¨ê³„] ëª©ì°¨ ì •ë°€ ì¶”ì¶œ (ë…¸ì´ì¦ˆ ì œê±° + í•©ì¹˜ê¸°)
def extract_master_terms():
    print("ğŸ“– [1ë‹¨ê³„] ëª©ì°¨ ì •ë°€ ì¶”ì¶œ ì¤‘...")
    term_list = []
    
    index_pattern = re.compile(r'^(?P<term>.*?)\s*[ï½¥ãƒ»\.]+\s*\d+$')
    noise_prefix_pattern = re.compile(r'^(ê²½ì œê¸ˆìœµìš©ì–´\s*\d*ì„ |ë³´ê¸°|ì°¸ê³ )\s*')

    with pdfplumber.open(PDF_FILE_PATH) as pdf:
        for i in range(INDEX_START_PAGE - 1, INDEX_END_PAGE):
            page = pdf.pages[i]
            width = page.width
            height = page.height
            
            left_box = (0, 60, width / 2, height - 50)
            right_box = (width / 2, 60, width, height - 50)
            
            for box in [left_box, right_box]:
                try:
                    text = page.crop(box).extract_text()
                except: continue
                if not text: continue
                
                lines = text.split('\n')
                prev_line = ""
                
                for line in lines:
                    clean_line = line.replace("ì°¾ì•„ë³´ê¸°", "").replace("ì°¾ì•„ë³´", "").replace("â™", "").strip()
                    if not clean_line: continue
                    
                    clean_line = noise_prefix_pattern.sub('', clean_line)

                    match = index_pattern.match(clean_line)
                    if match:
                        current_term = match.group('term').strip()
                        if prev_line:
                            full_term = f"{prev_line}{current_term}"
                            term_list.append(full_term)
                            prev_line = "" 
                        else:
                            if len(current_term) > 1:
                                term_list.append(current_term)
                    else:
                        if len(clean_line) > 1 and not clean_line.isdigit():
                            prev_line = clean_line

    unique_terms = list(dict.fromkeys(term_list))
    print(f"âœ… ëª©ì°¨ ì¶”ì¶œ ì™„ë£Œ: {len(unique_terms)}ê°œ ìš©ì–´ ê¸°ì¤€ í™•ë³´.")
    return unique_terms

# 6. [2ë‹¨ê³„] ë³¸ë¬¸ íŒŒì‹± ë° DB ì ì¬
def parse_and_insert_db():
    # DB ì´ˆê¸°í™”
    init_db_table()
    
    master_terms = extract_master_terms()
    normalized_master_set = set(normalize(t) for t in master_terms)
    
    print(f"ğŸ“‚ [2ë‹¨ê³„] ë³¸ë¬¸ ë¶„ì„ ë° DB ì ì¬ ì‹œì‘ (ì—„ê²©í•œ ì¼ì¹˜)...")
    
    data_list = [] 
    
    with pdfplumber.open(PDF_FILE_PATH) as pdf:
        current_title = ""
        current_body = ""
        
        for i, page in enumerate(pdf.pages):
            current_page_num = i + 1
            if current_page_num < BODY_START_PAGE: continue
            
            width, height = page.width, page.height
            try:
                cropped = page.crop((0, 80, width, height - 70))
                text = cropped.extract_text()
            except: continue

            if not text: continue

            lines = text.split('\n')
            for line in lines:
                clean_line = line.strip()
                if len(clean_line) < 1: continue
                if "ì—°ê´€ê²€ìƒ‰ì–´" in clean_line: continue

                norm_line = normalize(clean_line)
                is_title = norm_line in normalized_master_set

                if is_title:
                    if current_title and current_body:
                        # íŠœí”Œ í˜•íƒœë¡œ ì €ì¥ (execute_many ì‚¬ìš©ì„ ìœ„í•´)
                        data_list.append((current_title, current_body.strip()))
                    
                    current_title = clean_line
                    current_body = "" 
                else:
                    if "PDF.js" not in clean_line and not clean_line.isdigit():
                        current_body += " " + clean_line

            if current_page_num % 50 == 0:
                print(f"   ... {current_page_num}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘")

        if current_title and current_body:
            data_list.append((current_title, current_body.strip()))

    # DBì— ì¼ê´„ ì €ì¥ (Bulk Insert)
    if data_list:
        print(f"ğŸ’¾ ì´ {len(data_list)}ê°œ ë°ì´í„°ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤...")
        
        # [ë³€ê²½] execute_manyë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ëŸ‰ ì‚½ì…
        insert_sql = "INSERT INTO terms (word, definition) VALUES (%s, %s)"
        try:
            count = execute_many(insert_sql, data_list)
            print(f"ğŸ‰ ì„±ê³µì ìœ¼ë¡œ {count}ê°œì˜ ë°ì´í„°ê°€ DBì— ì ì¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì ì¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    parse_and_insert_db()
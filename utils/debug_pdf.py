import pdfplumber
import re
import os

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
PDF_FILE_PATH = os.path.join(BASE_DIR, "data", "economic_terms.pdf")
OUTPUT_FILE = os.path.join(BASE_DIR, "data", "final_verification_strict.txt")

# í˜ì´ì§€ ì„¤ì •
INDEX_START_PAGE = 5   
INDEX_END_PAGE = 16    
BODY_START_PAGE = 17   

# ì •ê·œí™” í•¨ìˆ˜: ë„ì–´ì“°ê¸°, íŠ¹ìˆ˜ë¬¸ì ë¬´ì‹œí•˜ê³  'ê¸€ì'ë§Œ ë¹„êµ
def normalize(text):
    if not text: return ""
    return re.sub(r'[\s\(\)\[\]\-\.,ï½¥ãƒ»/]', '', text)

# 1. ëª©ì°¨(Index) ì¶”ì¶œ - "ê³µë°± ì—†ì´ í•©ì¹˜ê¸°" ë¡œì§
def extract_master_terms():
    print("ğŸ“– [1ë‹¨ê³„] ëª©ì°¨ ì •ë°€ ì¶”ì¶œ (ê¸°ì¤€ì  í™•ë³´)...")
    term_list = []
    # ëª©ì°¨ íŒ¨í„´: "ìš©ì–´" + "ì ë“¤" + "ìˆ«ì"
    index_pattern = re.compile(r'^(?P<term>.*?)\s*[ï½¥ãƒ»\.]+\s*\d+$')

    with pdfplumber.open(PDF_FILE_PATH) as pdf:
        for i in range(INDEX_START_PAGE - 1, INDEX_END_PAGE):
            page = pdf.pages[i]
            width = page.width
            height = page.height
            
            # 2ë‹¨ ë¶„ë¦¬
            left_box = (0, 60, width / 2, height - 50)
            right_box = (width / 2, 60, width, height - 50)
            
            for box in [left_box, right_box]:
                try:
                    text = page.crop(box).extract_text()
                except: continue
                if not text: continue
                
                lines = text.split('\n')
                prev_line = ""
                
                for line in lines:
                    # ë…¸ì´ì¦ˆ ì œê±°
                    clean_line = line.replace("ì°¾ì•„ë³´ê¸°", "").replace("ì°¾ì•„ë³´", "").replace("â™", "").strip()
                    if not clean_line: continue

                    match = index_pattern.match(clean_line)
                    if match:
                        current_term = match.group('term').strip()
                        if prev_line:
                            # ì¤„ë°”ê¿ˆëœ ìš©ì–´ëŠ” ë¶™ì—¬ì„œ í•˜ë‚˜ë¡œ ë§Œë“¦
                            full_term = f"{prev_line}{current_term}"
                            term_list.append(full_term)
                            prev_line = "" 
                        else:
                            if len(current_term) > 1:
                                term_list.append(current_term)
                    else:
                        # ìˆ«ìë¡œ ëë‚˜ì§€ ì•ŠëŠ” ì¤„ (ìš©ì–´ì˜ ì•ë¶€ë¶„)
                        if len(clean_line) > 1 and not clean_line.isdigit():
                            prev_line = clean_line

    # ì¤‘ë³µ ì œê±°
    unique_terms = list(dict.fromkeys(term_list))
    print(f"âœ… ëª©ì°¨ ì¶”ì¶œ ì™„ë£Œ: {len(unique_terms)}ê°œ ìš©ì–´ ê¸°ì¤€")
    return unique_terms

# 2. ë³¸ë¬¸(Body) ê²€ì¦ - "ì—„ê²©í•œ ì¼ì¹˜(Strict Match)"
def verify_body_strict():
    master_terms = extract_master_terms()
    
    # ë¹„êµ ì†ë„ë¥¼ ìœ„í•´ ì •ê·œí™”ëœ ì…‹(Set) ìƒì„±
    normalized_master_set = set(normalize(t) for t in master_terms)
    
    print(f"ğŸ“– [2ë‹¨ê³„] ë³¸ë¬¸ ê²€ì¦ ë° íŒŒì¼ ìƒì„± ('{OUTPUT_FILE}')...")
    
    saved_count = 0
    
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        with pdfplumber.open(PDF_FILE_PATH) as pdf:
            current_title = ""
            current_body = ""
            
            for i, page in enumerate(pdf.pages):
                current_page_num = i + 1
                if current_page_num < BODY_START_PAGE: continue
                
                width, height = page.width, page.height
                try:
                    # ë³¸ë¬¸ ì˜ì—­ í¬ë¡­
                    cropped = page.crop((0, 80, width, height - 70))
                    text = cropped.extract_text()
                except: continue

                if not text: continue

                lines = text.split('\n')
                for line in lines:
                    clean_line = line.strip()
                    if len(clean_line) < 1: continue
                    if "ì—°ê´€ê²€ìƒ‰ì–´" in clean_line: continue

                    # í˜„ì¬ ë¼ì¸ ì •ê·œí™”
                    norm_line = normalize(clean_line)
                    
                    # ğŸ”¥ [ì—„ê²©í•œ ë¡œì§] ì •ê·œí™”ëœ ë¼ì¸ì´ ëª©ì°¨ ì…‹ì— 'ì •í™•íˆ' ìˆëŠ”ê°€?
                    is_title = norm_line in normalized_master_set

                    if is_title:
                        # ì´ì „ì— ì‘ì—…í•˜ë˜ ìš©ì–´ ì €ì¥
                        if current_title and current_body:
                            f.write(f"[{current_title}]\n")
                            f.write(f"{current_body.strip()}\n")
                            f.write("-" * 50 + "\n")
                            saved_count += 1
                        
                        # ìƒˆ ìš©ì–´ ì‹œì‘
                        current_title = clean_line
                        current_body = "" # ì œëª© ì¤„ì€ ë³¸ë¬¸ì— ë„£ì§€ ì•ŠìŒ
                    else:
                        # ì œëª©ì´ ì•„ë‹ˆë©´ ë¬´ì¡°ê±´ ë³¸ë¬¸
                        if "PDF.js" not in clean_line and not clean_line.isdigit():
                            current_body += " " + clean_line

                if current_page_num % 50 == 0:
                    print(f"   ... {current_page_num}í˜ì´ì§€")

            # ë§ˆì§€ë§‰ ìš©ì–´ ì €ì¥
            if current_title and current_body:
                f.write(f"[{current_title}]\n")
                f.write(f"{current_body.strip()}\n")
                f.write("-" * 50 + "\n")
                saved_count += 1

    print(f"ğŸ‰ ê²€ì¦ ì™„ë£Œ! ì´ {saved_count}ê°œì˜ ìš©ì–´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ‘‰ '{OUTPUT_FILE}' íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    verify_body_strict()
import os
import speech_recognition as sr
from openai import OpenAI
from dotenv import load_dotenv  # ì¶”ê°€ëœ ë¶€ë¶„

# 1. .env íŒŒì¼ ë¡œë“œ
# ì´ í•¨ìˆ˜ê°€ ì‹¤í–‰ë˜ë©´ .env íŒŒì¼ì˜ ë‚´ìš©ì´ í™˜ê²½ ë³€ìˆ˜ë¡œ ë“±ë¡ë©ë‹ˆë‹¤.
load_dotenv()

# 2. í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
api_key = os.getenv("OPENAI_API_KEY")

# í‚¤ê°€ ì œëŒ€ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸ (ë””ë²„ê¹…ìš©)
if not api_key:
    raise ValueError("âŒ .env íŒŒì¼ì—ì„œ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = OpenAI(api_key=api_key)

def listen_from_mic():
    """
    ë§ˆì´í¬ë¡œë¶€í„° ìŒì„±ì„ ë“£ê³  ì„ì‹œ wav íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    r = sr.Recognizer()
    
    with sr.Microphone() as source:
        print("ğŸ¤ ë§ì”€í•´ ì£¼ì„¸ìš” (ë“£ê³  ìˆìŠµë‹ˆë‹¤...)")
        
        # ë°°ê²½ ì†ŒìŒ ìˆ˜ì¤€ì„ ì¡°ì •í•˜ì—¬ ì •í™•ë„ í–¥ìƒ
        r.adjust_for_ambient_noise(source)
        
        try:
            # ìŒì„± ê°ì§€ ë° ë…¹ìŒ (íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€: 5ì´ˆ ë™ì•ˆ ë§ ì—†ìœ¼ë©´ ì¢…ë£Œ)
            audio = r.listen(source, timeout=5, phrase_time_limit=10)
            print("âœ… ë…¹ìŒ ì™„ë£Œ! ë³€í™˜ ì¤‘...")
            
            # Whisper APIëŠ” íŒŒì¼ í˜•íƒœë¥¼ ìš”êµ¬í•˜ë¯€ë¡œ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            filename = "my_voice.wav"
            with open(filename, "wb") as f:
                f.write(audio.get_wav_data())
                
            return filename
        except sr.WaitTimeoutError:
            print("â³ ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

def transcribe_audio(filename):
    """
    ì €ì¥ëœ ì˜¤ë””ì˜¤ íŒŒì¼ì„ OpenAI Whisper APIë¡œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    if not filename: return None
    
    try:
        with open(filename, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1", 
                file=audio_file,
                # language="ko" # í•„ìš” ì‹œ ì–¸ì–´ ê°•ì œ ì„¤ì • ê°€ëŠ¥
            )
        return transcript.text
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ (STT): {e}")
        return None

def ask_llm(text):
    """
    ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ LLM(GPT)ì—ê²Œ ë³´ë‚´ê³  ë‹µë³€ì„ ë°›ëŠ” í•¨ìˆ˜
    """
    try:
        with open("prompt/mic_system_prompt.md", "r", encoding="utf-8") as f:
                system_prompt_content = f.read()
            

        # 2. API í˜¸ì¶œì— ì ìš©
        response = client.chat.completions.create(
            model="gpt-4o-mini", 
            messages=[
                {
                    "role": "system", 
                    "content": system_prompt_content
                },
                {
                    "role": "user", 
                    "content": text
                }
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ (LLM): {e}")
        return None

# --- ë©”ì¸ ì‹¤í–‰ íë¦„ ---
if __name__ == "__main__":
    # 1. ë§ˆì´í¬ ì…ë ¥ ë°›ê¸°
    audio_file = listen_from_mic()
    
    if audio_file:
        # 2. ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Whisper)
        user_prompt = transcribe_audio(audio_file)
        
        if user_prompt:
            print(f"\nğŸ—£ï¸ ì¸ì‹ëœ ì§ˆë¬¸: {user_prompt}\n")
            print("-" * 30)
            
            # 3. LLMì—ê²Œ ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸° (GPT)
            ai_response = ask_llm(user_prompt)
            
            if ai_response:
                print(f"ğŸ¤– AI ë‹µë³€:\n{ai_response}")
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(audio_file):
                os.remove(audio_file)
        else:
            print("ìŒì„± ì¸ì‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")